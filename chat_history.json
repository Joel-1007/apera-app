[{"role": "user", "content": "what is Machine Learning and deep learning "}, {"role": "assistant", "content": "Machine Learning (ML) is a subset of Artificial Intelligence that involves training algorithms to learn patterns and make decisions based on data, without being explicitly programmed for each task. Deep Learning is a specific type of machine learning that uses artificial neural networks with multiple layers to learn more complex patterns from large amounts of data. The deep neural networks, which were initiated as breakthrough advancements in 2012, have led to significant improvements in predictive accuracy across various classification tasks, and are now the foundational architecture for a range of technologies like hiring algorithms, facial recognition systems, content moderation tools, and text generation systems. These developments laid the groundwork for systems that generate text, images, and other outputs that users often find coherent and contextually appropriate.", "citations": [{"text": "Further, some researchers view AI as an overarching field \nthat includes subdomains like natural language processing, computer vision, speech recognition, \nand robotics. Some, however, dismiss these as overly \u201cnarrow\u201d definitions, instead associating AI \nwith the still-unrealized goal of machines possessing human-level or even superhuman \n4 \nintelligence. As a result, AI remains a fluid and ambiguous term, with any attempt to pin down a \nspecific definition inherently open to critique (Kuhlmann et al., 2019; Schopmans, 2022). For the \npurposes of this paper, however, AI is conceptualized by reference to the breakthrough \nadvancements initiated in 2012, when deep neural networks (a specific class of algorithms) started \ndelivering significantly better results than earlier approaches across various classification tasks \n(Krizhevsky, Sutskever, & Hinton, 2012; LeCun, Bengio, & Hinton, 2015). These algorithms, \nrepresenting a specific subset of machine learning models, achieved unprecedented improvements \nin predictive accuracy across a wide range of classification tasks (Cupa\u0107 et al., 2024, p. 903). \nUnlike earlier computational approaches that relied on explicitly programmed rules, deep neural \nnetworks are trained on vast datasets comprising millions, billions, or even trillions of data points. \nThis capacity enables them to identify complex patterns and make decisions autonomously, guided \nonly by the system\u2019s goals. Over the course of a decade, deep learning has evolved into the \nfoundational architecture for a range of technologies, including hiring and social media algorithms, \nfacial recognition systems, and content moderation tools, fundamentally transforming both the \noperation of these systems and the ways in which users interact with them. \n \nDespite being hailed as a transformative technology, AI is fundamentally conservative in nature, \nrelying on historical data to learn and make decisions. Rather than generating entirely new patterns, \nAI systems tend to reproduce\u2014and often entrench\u2014existing societal structures and biases. When \nthe underlying data reflects longstanding inequalities\u2014whether based on gender, race, class, or \nother categories\u2014AI systems inevitably learn and replicate these biases (Bengio et al., 2025).", "file": "Local DB", "score": 0.6456758719636355, "rank": 0, "type": "local"}, {"text": "2018). Scaling these models \nproduced unexpected capabilities: without explicit task-specific instruction, they \nbegan to translate, summarise, reason, and compose text (Bommasani et al. 2021). \nThese emergent behaviours reshaped understandings of how model size, data \nscale, and architectural design interact. These developments laid the groundwork \nfor systems that generate text, images, and other outputs that users often find \ncoherent and contextually appropriate. \nIn summary, \u2018generative AI\u2019 refers to a family of techniques within machine \nlearning. Ethically, this means that concerns and promises of other machine \nlearning approaches are likely also found in the case of generative AI. But there is \nalso more. Unlike traditional predictive machine learning systems, which classify \ninputs or estimate probabilities, generative systems model the structure of \nhuman-created data so well that they can produce convincing simulations of \nhuman expression which, I suggested, invites us to experience these as real, or \nhuman. In the next section, I discuss three developments that further contribute \nto this affordance that are not strictly new technological breakthroughs but ways \nin which generative AI is used, deployed, and connected with other technologies. \n \nMultimodal and agentic extensions \nBeyond these underlying technical developments, several changes in how \ngenerative AI is deployed have made the systems more salient in everyday life. \nOne is the design of user interfaces that allow individuals without technical \nexpertise to interact with complex models in familiar ways, often through ordinary \nlanguage. Another is the integration of multiple modalities such as text, images, \nEthics of Generative AI \nEncyclopedia of Applied Ethics \n6 \n \naudio, and video. into unified systems that can produce or interpret several kinds \nof content during a single interaction. A third is the incorporation of planning and \ntool-use capabilities, which enable systems to carry out multi-step tasks that give \nthe appearance of coordinated, goal-directed activity. \nThese developments do not entail that generative AI systems possess the \nmental states or forms of understanding characteristic of human agents. What \nthey do is expand the range of contexts in which users may find it natural to treat \nsystem outputs as if they expressed intentions, beliefs, or other attitudes.", "file": "Local DB", "score": 0.5691048011586186, "rank": 0, "type": "local"}, {"text": "[11] Laurence Dierickx, S. Sir\u00b4en-Heikel, and C. G.\nLind\u00b4en. Outsourcing, augmenting, or complicat-\ning: The dynamics of ai in fact-checking prac-\ntices in the nordics. Emerging Media, 2024. doi:\n10.1177/27523543241288846.\n[12] A. H. Ekeh, C. E. Apeh, and C. S. Odionu. Lever-\naging machine learning for environmental policy\ninnovation: Advances in data analytics to address\nurban and ecological challenges. Gulf Journal of\nAdvanced Business Research, 2(1), 2025. URL\nhttps://fegulf.com/index.php/gjabr/art\nicle/view/92.\n[13] Eduardo Fernandes, S\u00b4ergio Moro, and Paulo\nCortez. Data science, machine learning and big\ndata in digital journalism: A survey of state-\nof-the-art, challenges and opportunities. Expert\nSystems with Applications, 215:119187, 2023. doi:\n10.1016/j.eswa.2023.119187.\n[14] Luciano Floridi. Ethics, Governance, and Poli-\ncies in Artificial Intelligence. Springer, 2021. doi:\n10.1007/978-3-030-81907-1.\n[15] Luciano Floridi.\nThe Ethics of Artificial In-\ntelligence: Principles, Challenges, and Oppor-\ntunities. Oxford University Press, 2023. URL\nhttps://books.google.com/books?hl=en&i\nd=_f3KEAAAQBAJ.\n[16] P. Garc\u00b4\u0131a-Tapia-Mateo and A. Bueno-Crespo.\nExplainable deep learning for early detection of\nnatural disasters through social media text clas-\nsification. SSRN Electronic Journal, 2025. URL\nhttps://papers.ssrn.com/sol3/papers.cf\nm?abstract_id=5113748.\n20\n[17] Gregory Gondwe. Artificial intelligence, journal-\nism, and the ubuntu robot in sub-saharan africa:\nTowards a normative framework.", "file": "Local DB", "score": 0.5690364127014063, "rank": 0, "type": "local"}, {"text": "Actually, it\u2019s\nabout ethics, ai, and journalism: Reporting on\nand with computation and data. Technical report,\nTow Center for Digital Journalism, Columbia\nUniversity, 2019. URL https://www.cjr.org/\ntow_center_reports/ai-ethics-journalism.\nphp.\n[22] B. Kazaz, S. Poddar, S. Arabi, M. A. Perez, and\nA. Sharma. Deep learning-based object detection\nfor unmanned aerial systems (uass)-based inspec-\ntions of construction stormwater practices. Sen-\nsors, 21(8):2834, 2021. doi: 10.3390/s21082834.\n[23] O. Korostin. Optimising machine learning in-\ntegration in real-time text analytics platforms:\nTechnical approaches and performance criteria.\nComputer-Integrated Technologies, 4(50), 2025.\nURL https://cit.lntu.edu.ua/index.php/\ncit/article/view/682.\n[24] Ruoyu Li. Research of improved detr models and\ntransformer applications in computer vision. In\nProceedings of the 2024 2nd International Con-\nference on Image, Algorithms and Artificial Intel-\nligence (ICIAAI 2024), pages 981\u2013992. Atlantis\nPress, 2024. doi: 10.2991/978-94-6463-540-9 98.\n[25] S. R. O. Marshall, T. N. D. Tran, and M. R.\nTapas. Integrating artificial intelligence and ma-\nchine learning in hydrological modeling for sus-\ntainable resource management.\nInternational\nJournal of Sustainable Development and Plan-\nning, 2025. doi: 10.1080/15715124.2025.2478280.\n[26] Philipp Mayring and Tobias Fenzl. Qualitative\ninhaltsanalyse. In N. Baur and J. Blasius, edi-\ntors, Handbuch Methoden der empirischen Sozial-\nforschung, pages 633\u2013648. Springer, 2019. doi:\n10.1007/978-3-658-21308-4 42.", "file": "Local DB", "score": 0.5536864485413711, "rank": 0, "type": "local"}, {"text": "2019. Energy and pol-\nicy consideration for deep learning in NLP. arXiv:1906.02243v1. \nUtz, V. and DiPaola, S. 2023. Climate implications of diffusion-\nbased generative visual AI systems and their mass adoption. Pro-\nceedings of the 14th International Conference on Computational \nCreativity: 264-272 \nYarbrough, A. C.; Harris, G. A.; and Purdy, G. T. 2022. Improving \nthe flow of data and information in manufacturing. Manufacturing \nLetter, 32:1. doi.org/10.1016/j.mfglet.2022.01.001", "file": "Local DB", "score": 0.523313757536495, "rank": 0, "type": "local"}], "mode": "Local DB (Hybrid FAISS+BM25)", "meta": {"intent": "TECHNICAL", "confidence": 59.5, "fairness": {"western_dominance_pct": 0.0, "balance_label": "Balanced", "diversity_flag": false}, "xai_reason": "Top match 'Local DB' score: 0.65"}}, {"role": "user", "content": "what is literature baselines "}, {"role": "user", "content": "what is ML models "}, {"role": "user", "content": "what is local baselines"}, {"role": "user", "content": "what is ML "}, {"role": "assistant", "content": " Machine Learning (ML) refers to a subset of artificial intelligence (AI) that provides systems the ability to automatically learn and improve from experience without being explicitly programmed. It focuses on the development of computer programs that can access data and use it to learn for themselves.", "citations": [{"text": "The COVID-19 pandemic and other ongoing health crises have underscored the need for prompt healthcare services worldwide. The traditional healthcare system, centered around hospitals and clinics, has proven inadequate in the face of such challenges. Intelligent wearable devices, a key part of modern healthcare, leverage Internet of Things technology to collect extensive data related to the environment as well as psychological, behavioral, and physical health. However, managing the substantial data generated by these wearables and other IoT devices in healthcare poses a significant challenge, potentially impeding decision-making processes. Recent interest has grown in applying data analytics for extracting information, gaining insights, and making predictions. Additionally, machine learning, known for addressing various big data and networking challenges, has seen increased implementation to enhance IoT systems in healthcare. This chapter focuses exclusively on exploring the hurdles encountered when integrating ML methods into the IoT healthcare sector. It offers a comprehensive summary of current research challenges and potential opportunities, categorized into three scenarios: IoT-based, ML-based, and the implementation of machine learning methodologies in the IoT-based healthcare industry. This compilation will assist future researchers, healthcare professionals, and government agencies by offering valuable insights into recent smart healthcare advancements.", "file": "Towards Smart Healthcare: Challenges and Opportunities in IoT and ML", "type": "arxiv"}, {"text": "Machine learning (ML) transforms healthcare by enabling predictive analytics, personalized treatments, and improved patient outcomes. However, traditional ML workflows often require specialized skills, infrastructure, and resources, limiting accessibility for many healthcare professionals. This paper explores how BigQuery ML Cloud service helps healthcare researchers and data analysts to build and deploy models using SQL, without need for advanced ML knowledge. Our results demonstrate that the Boosted Tree model achieved the highest performance among the three models making it highly effective for diabetes prediction. BigQuery ML directly integrates predictive analytics into their workflows to inform decision-making and support patient care. We reveal this capability through a case study on diabetes prediction using the Diabetes Health Indicators Dataset. Our study underscores BigQuery ML's role in democratizing machine learning, enabling faster, scalable, and efficient predictive analytics that can directly enhance healthcare decision-making processes. This study aims to bridge the gap between advanced machine learning and practical healthcare analytics by providing detailed insights into BigQuery ML's capabilities. By demonstrating its utility in a real-world case study, we highlight its potential to simplify complex workflows and expand access to predictive tools for a broader audience of healthcare professionals.", "file": "Machine Learning for Everyone: Simplifying Healthcare Analytics with BigQuery ML", "type": "arxiv"}, {"text": "We implement a library for encoding constructor classes in Standard ML, including elaboration from minimal definitions, and automatic instantiation of superclasses.", "file": "Close Encounters of the Higher Kind Emulating Constructor Classes in Standard ML", "type": "arxiv"}], "mode": "Live Research (ArXiv)", "meta": {"intent": "RESEARCH", "fairness": {"western_selection_rate": 0.0, "fairlearn_status": "Balanced", "balance_label": "Balanced"}, "confidence": 85, "xai_reason": "Based on 3 sources from ArXiv"}}, {"role": "user", "content": "how would you search for proper bias across multiple research papers"}, {"role": "assistant", "content": " To search for proper bias across multiple research papers, you could use a combination of search engines and keywords related to the topic. Here's an example query using Google Scholar:\n\n`\"stereotypical bias\" AND (\"pre-trained language models\" OR \"English language models\") AND (\"multiple languages\" OR \"multilingual models\")`\n\nThis query should help you find research papers that discuss stereotypical bias in pre-trained language models across multiple languages, which seems to be the focus of the context provided. You can also adjust the keywords and combine them as needed to fine-tune your search results. Additionally, you may want to explore academic databases like PubMed or JSTOR for more specialized searches.", "citations": [{"text": "Scientific writing is an important skill for a career as a professional astrophysicist. Very few researchers, however, receive any formal training in how to write scientific research papers of high quality in an efficient manner. This paper (Paper I) is the first of a two-part self-help guide in scientific writing to address this skills gap. Paper I focuses on planning your academic research paper in astronomy. We discuss how to crystallise the ideas that underlie a research project, analyse how the paper can be constructed considering the audience and the chosen journal, and give an overview of the publishing process. Paper II is a detailed description of the different sections that make up a research paper in astronomy and shares the best practice in how to write in English. Whether you are a student writing your first paper or an experienced author, you may find the ideas presented here useful.", "file": "How to plan your astronomy research paper in ten steps", "type": "arxiv"}, {"text": "Researchers spend a great deal of time reading research papers. Keshav (2012) provides a three-pass method to researchers to improve their reading skills. This article extends Keshav's method for reading a research compendium. Research compendia are an increasingly used form of publication, which packages not only the research paper's text and figures, but also all data and software for better reproducibility. We introduce the existing conventions for research compendia and suggest how to utilise their shared properties in a structured reading process. Unlike the original, this article is not build upon a long history but intends to provide guidance at the outset of an emerging practice.", "file": "How to Read a Research Compendium", "type": "arxiv"}, {"text": "Recent studies have demonstrated how to assess the stereotypical bias in pre-trained English language models. In this work, we extend this branch of research in multiple different dimensions by systematically investigating (a) mono- and multilingual models of (b) different underlying architectures with respect to their bias in (c) multiple different languages. To that end, we make use of the English StereoSet data set (Nadeem et al., 2021), which we semi-automatically translate into German, French, Spanish, and Turkish. We find that it is of major importance to conduct this type of analysis in a multilingual setting, as our experiments show a much more nuanced picture as well as notable differences from the English-only analysis. The main takeaways from our analysis are that mGPT-2 (partly) shows surprising anti-stereotypical behavior across languages, English (monolingual) models exhibit the strongest bias, and the stereotypes reflected in the data set are least present in Turkish models. Finally, we release our codebase alongside the translated data sets and practical guidelines for the semi-automatic translation to encourage a further extension of our work to other languages.", "file": "How Different Is Stereotypical Bias Across Languages?", "type": "arxiv"}], "mode": "Live Research (ArXiv)", "meta": {"intent": "RESEARCH", "fairness": {"western_selection_rate": 0.0, "fairlearn_status": "Balanced", "balance_label": "Balanced"}, "confidence": 85, "xai_reason": "Based on 3 sources from ArXiv"}}, {"role": "user", "content": "but the terminology for bias has varied across different years so which one is arguable the best definition"}, {"role": "assistant", "content": " The terminology for \"bias\" can indeed vary across different contexts and disciplines. In general, bias refers to a systematic error or distortion in data collection, interpretation, or analysis that can lead to incorrect conclusions. Here's a broad definition that is often used in statistics and research:\n\nBias: A deviation of results from the truth due to factors influencing the design, conduct, or analysis of a study, experiment, survey, or observational study. Bias arises when there is systematic difference between the observed frequency distribution and the true underlying distribution.\n\nHowever, it's important to note that definitions can vary based on the specific field or application. For instance, in machine learning, bias might refer to the tendency of an algorithm to favor certain inputs or outcomes over others, often due to the data used for training. In social sciences, bias could also refer to personal or cultural prejudices that influence one's interpretation of events or people.\n\nIn your question, you seemed to suggest that there might be different definitions across different years. This is plausible as terminology can evolve over time due to changes in academic discourse, technology, and societal understanding. For a more specific or precise definition tailored to a particular year or field, I would recommend referring to authoritative sources within that domain.", "citations": [{"text": "On 2017 August 17 a binary neutron star coalescence candidate (later designated GW170817) with merger time 12:41:04 UTC was observed through gravitational waves by the Advanced LIGO and Advanced Virgo detectors. The Fermi Gamma-ray Burst Monitor independently detected a gamma-ray burst (GRB 170817A) with a time delay of $\\sim$1.7 s with respect to the merger time. From the gravitational-wave signal, the source was initially localized to a sky region of 31 deg$^2$ at a luminosity distance of $40^{+8}_{-8}$ Mpc and with component masses consistent with neutron stars. The component masses were later measured to be in the range 0.86 to 2.26 Msun. An extensive observing campaign was launched across the electromagnetic spectrum leading to the discovery of a bright optical transient (SSS17a, now with the IAU identification of AT 2017gfo) in NGC 4993 (at $\\sim$40 Mpc) less than 11 hours after the merger by the One-Meter, Two Hemisphere (1M2H) team using the 1 m Swope Telescope. The optical transient was independently detected by multiple teams within an hour. Subsequent observations targeted the object and its environment. Early ultraviolet observations revealed a blue transient that faded within 48 hours. Optical and infrared observations showed a redward evolution over $\\sim$10 days. Following early non-detections, X-ray and radio emission were discovered at the transient's position $\\sim$9 and $\\sim$16 days, respectively, after the merger. Both the X-ray and radio emission likely arise from a physical process that is distinct from the one that generates the UV/optical/near-infrared emission. No ultra-high-energy gamma-rays and no neutrino candidates consistent with the source were found in follow-up searches. (Abridged)", "file": "Multi-messenger Observations of a Binary Neutron Star Merger", "type": "arxiv"}, {"text": "This paper has been withdrawn", "file": "This paper has been withdrawn", "type": "arxiv"}, {"text": "This paper examines the impact of Generative Artificial Intelligence (GenAI) tools like ChatGPT on the creation and consumption of terminological definitions. From the terminologist's point of view, the strategic use of GenAI tools can streamline the process of crafting definitions, reducing both time and effort, while potentially enhancing quality. GenAI tools enable AI-assisted terminography, notably post-editing terminography, where the machine produces a definition that the terminologist then corrects or refines. However, the potential of GenAI tools to fulfill all the terminological needs of a user, including term definitions, challenges the very existence of terminological definitions and resources as we know them. Unlike terminological definitions, GenAI tools can describe the knowledge activated by a term in a specific context. However, a main drawback of these tools is that their output can contain errors. For this reason, users requiring reliability will likely still resort to terminological resources for definitions. Nevertheless, with the inevitable integration of AI into terminology work, the distinction between human-created and AI-created content will become increasingly blurred.", "file": "What Generative Artificial Intelligence Means for Terminological Definitions", "type": "arxiv"}], "mode": "Live Research (ArXiv)", "meta": {"intent": "RESEARCH", "fairness": {"western_selection_rate": 0.0, "fairlearn_status": "Balanced", "balance_label": "Balanced"}, "confidence": 85, "xai_reason": "Based on 3 sources from ArXiv"}}, {"role": "user", "content": "exact answer"}, {"role": "assistant", "content": " The paper you provided discusses the development of a modified grand canonical ensemble to study the thermodynamic properties of hard sphere fluid-like systems confined in a spherical cavity. It does not seem to be directly related to Community Question Answering (cQA) or education, but rather focused on physics and statistical mechanics. The Community Question Answering context mentioned is from another part of the text you provided, which discusses SemEval-2015 Task 3 on \"Answer Selection in cQA\". In that context, the exact answer would be related to the subtasks of classifying answers as \"good\", \"bad\", or \"potentially relevant\" with respect to the question, and answering a YES/NO question with \"yes\", \"no\", or \"unsure\".", "citations": [{"text": "In education, open-ended quiz questions have become an important tool for assessing the knowledge of students. Yet, manually preparing such questions is a tedious task, and thus automatic question generation has been proposed as a possible alternative. So far, the vast majority of research has focused on generating the question text, relying on question answering datasets with readily picked answers, and the problem of how to come up with answer candidates in the first place has been largely ignored. Here, we aim to bridge this gap. In particular, we propose a model that can generate a specified number of answer candidates for a given passage of text, which can then be used by instructors to write questions manually or can be passed as an input to automatic answer-aware question generators. Our experiments show that our proposed answer candidate generation model outperforms several baselines.", "file": "Generating Answer Candidates for Quizzes and Answer-Aware Question Generators", "type": "arxiv"}, {"text": "This paper presents a modified grand canonical ensemble which provides a new simple and efficient scheme to study few-body fluid-like inhomogeneous systems under confinement. The new formalism is implemented to investigate the exact thermodynamic properties of a hard sphere (HS) fluid-like system with up to three particles confined in a spherical cavity. In addition, the partition function of this system was used to analyze the surface thermodynamic properties of the many-HS system and to derive the exact curvature dependence of both the surface tension and adsorption in powers of the density. The expressions for the surface tension and the adsorption were also obtained for the many- HS system outside of a fixed hard spherical object. We used these results to derive the dependence of the fluid-substrate Tolman length up to first order in density.", "file": "An exact formalism to study the thermodynamic properties of hard-sphere systems under spherical confinement", "type": "arxiv"}, {"text": "Community Question Answering (cQA) provides new interesting research directions to the traditional Question Answering (QA) field, e.g., the exploitation of the interaction between users and the structure of related posts. In this context, we organized SemEval-2015 Task 3 on \"Answer Selection in cQA\", which included two subtasks: (a) classifying answers as \"good\", \"bad\", or \"potentially relevant\" with respect to the question, and (b) answering a YES/NO question with \"yes\", \"no\", or \"unsure\", based on the list of all answers. We set subtask A for Arabic and English on two relatively different cQA domains, i.e., the Qatar Living website for English, and a Quran-related website for Arabic. We used crowdsourcing on Amazon Mechanical Turk to label a large English training dataset, which we released to the research community. Thirteen teams participated in the challenge with a total of 61 submissions: 24 primary and 37 contrastive. The best systems achieved an official score (macro-averaged F1) of 57.19 and 63.7 for the English subtasks A and B, and 78.55 for the Arabic subtask A.", "file": "SemEval-2015 Task 3: Answer Selection in Community Question Answering", "type": "arxiv"}], "mode": "Live Research (ArXiv)", "meta": {"intent": "RESEARCH", "fairness": {"western_selection_rate": 0.0, "fairlearn_status": "Balanced", "balance_label": "Balanced"}, "confidence": 85, "xai_reason": "Based on 3 sources from ArXiv"}}]